---
title: "o2r Reproducibility Service Load Testing"
author: "Juan Sebastian Garzon Alvarado, Daniel Nüst"
date: "3/27/2020"
output: md_document
---
# o2r Reproducibility Service Load Testing

In order to evaluate the performance of the o2r reproducibility service, we conduct a suite of load tests for expected usage scenarios: an **ERC creation scenario**, an **ERC examination scenario**, and a **combined scenario**. The data collected during these load tests allows operators of the reproducibility service to estimate the required infrastructure and helps developers to improve the performance. Each section of the document contains the functions required to test the o2r service both in `local` implementation and in the `remote` [o2r.uni-muenster.de](o2r.uni-muenster.de) implementation.

The ERC creation scenario assumes that a number of authors across all time zones prepare an ERC-based submission to a special issue. A special issue call is usually open for ~ 6 months, but often has a high number of submissions close to the deadline. Because of the temporal distribution and the unclear expected activity, we assume **3 parallel creation sessions**.

The number of sessions for the examination scenario is aligned with a typical number of papers in a special issue, i.e. 10 papers, and simulates the case of two concurrent readers per paper, i.e. viewing sessions 20 in total per issue. The test evaluates the performance of the API separately for the first two scenarios.

Finally, a third scenario evaluates the parallel execution of the first two scenarios. This would be the case for a reproducibility service operated by a publisher who continuously runs special issues, **having both readers** and open calls for submissions.

## Load testing basics

TODO: what is load testing? (how is not) add some links/literature here

## Creation scenario

### Overview

From the Author perspective the test simulates four main steps:

1. Service authentication

2. Upload of either a workspace or an ERC via either direct file upload or from one of two supported public shares

3. Metadata editing

4. Compendium execution and bindings testing

Every creation session has two main variables: the source and the type of upload. Six combinations of upload type and origin origin are possible in the first step. As the objective is to simulate three parallel cases, the creation scenario will have subsequent creation sessions to cover all the possible cases.

TODO: how often do we run each combinations to get trustworthy data?

### Upload type and origin

The test differentiates between the **two types of uploads**, which can either be a workspace or a complete ERC. A workspace is an archive (.zip Format) of all files needed for a scientific workflow, i.e. the main file in R Markdown format, e.g., main.Rmd, the file for reading, e.g., display.html, and all additionally required data and software files (see ERC specification for details). The finished ERC in addition contains the ERC configuration file (erc.yml) and the files for the computing environment (the manifest in a Dockerfile, the image in image.tar).

The type of upload is configured via the `content_type` parameter for upload requests, see http://o2r.info/api/compendium/upload/.

It is of interest if one of the formats significantly impact performance, e.g., because the file sizes of ERC are much bigger because of the image tarball, while the processing is reduced since the manifest and image must not be created during submission.

The test includes three types of **upload origins**:

1. [Local file upload](http://o2r.info/api/compendium/upload/)
2. [Public Share from Zenodo](http://o2r.info/api/compendium/public_share/#zenodo)
3. [Public Share from Sciebo](http://o2r.info/api/compendium/public_share/#sciebo)

It is of interest if any of these sources perform better or worse than others. For the Zenodo option, only the Record ID identification is used.

### Metadata editing

As described on the compendium life cycle, the ‘candidate process’ should be applied to the compendium or workspaces in order to be publicly available. This process should include a modifiable time that the user would take interacting with the platform. Each user just saves the metadata once. After metadata editing the ERC is published.

TODO: document task (https://github.com/o2r-project/api/issues) for later: have authors save metadata several times before publishing

### Execution and interaction 

After the publication of the compendiums the authors will most likely also execute the ERCs and test the interaction with bindings, if the latter exist. Therefore at least one execution job is started after each upload, see  http://o2r.info/api/job/#execute-a-compendium.

## Examination scenario

### Overview

The reading sessions consist on the simulation of 20 readers accessing and interacting with existing Compendiums (i.e. successfully created Compendia). The reading session will access to a compendium and execute a job (see ##Job). For the test it is required to have at least 10 compendia. The time of start of the execution of each individual examination session should be adjustable. 

From the reader perspective the test simulates two main steps:

1. Service authentication

2. Compendium execution and bindings testing

## Implementation

### Overview

The test plan will be scripted using R and is controlled via an R Markdown notebook (this file). The test notebook is part of the API documentation and is published at https://o2r.info/api/evaluation.

### Summary of functions

The test includes a series of functions to conduct the usage scenarios including the required processes divided in 4 groups (1) Service Authentication, (2) ERC / Workspace Upload, (3) Metadata editing (i.e. [Candidate process](http://o2r.info/api/compendium/candidate/)) and (4) Compendium execution.

### Required libraries

```{r libraries}
library(RSelenium)
library(binman)
library(httr)
library(rjson)
library(stringr)
library(parallel)
library(tidyr)
```

### 1. Service Authentication

#### Remote service

In case of a remote service [o2r.uni-muenster.de](http://o2r.uni-muenster.de) this chunck access to the demo webpage and find the cookie `connec.sid` after a succesfull login. It is required a `.Renviron` file on your local machine next to this file defining USERNAME and PASSWORD variables corresponding to the login information of [orcid](orcidorcid.org) **personal account**.

```{r get_cookie_remote}

#Function to assign in the local environment the Cookie 'connec.sid' and the endpoint corresponding to the type of test (local or remote)

get_cookie_remote<-function(){
  
  #Remote Test
  o2r_remote<-"https://o2r.uni-muenster.de/#/"
  
  # Read configuration file
  readRenviron(".Renviron")

  available.versions<-list_versions("chromedriver")
  
  r<-rsDriver(chromever=available.versions[[1]][5],port=sample(4500:4999,1))
  rem_dr<-r[["client"]]  
  
  # o2r webpage
  rem_dr$navigate(o2r_remote)
  web_elem<-rem_dr$findElement(using = "xpath", "//a[@href='api/v1/auth/login']")
  web_elem$clickElement()
  Sys.sleep(1)
  # orcid.org login webpage - Personal login
  
  web_elem_username<-rem_dr$findElement(using="id",value='userId')
  Sys.sleep(1)
  web_elem_pass<-rem_dr$findElement(using="id",value='password')
  Sys.sleep(1)
  web_elem_username$sendKeysToElement(list(Sys.getenv("USERNAME")))
  Sys.sleep(1)
  web_elem_pass$sendKeysToElement(list(Sys.getenv("PASSWORD"),key="enter"))
  Sys.sleep(1)
  
  # Get cookie
  cookie<-URLdecode(rem_dr$getAllCookies()[[1]]$value)
  Sys.sleep(1)
  Sys.setenv(COOKIE=cookie)
  Sys.setenv(ENDPOINT="https://o2r.uni-muenster.de/api/v1/")
  print(cookie)
  #Close
  rem_dr$close()
  rm(r)
}
```

#### Local service 

The following code chunk retrieves the cookie `connec.sid`from the local reference implementation, which is exposed via [o2r-guestlister](). This is a "security hole" which of course does not work when uploading workspaces to a remote reference implementation deployment. It is required a  `.Renviron` file on your local machine next to this file defining `IP` corresponding to the o2r ip address of your Docker environment inplementation. To find the ip (Usually but not always `localhost`) you have to use the `$docker-machine ip` command.

```{r get_cookie_local}

get_cookie_local<-function(){
  
  # LocalTest
  o2r_local<-Sys.getenv("IP")
  
  # Starting Selenium session
  available.versions<-list_versions("chromedriver")
  r<-rsDriver(port=1234L,chromever=available.versions[[1]][1])
  rem_dr<-r[["client"]]  
  
  # Navigating to o2r local implementation page
  
  rem_dr$navigate(o2r_local)
  
  # Click on Login
  web_elem<-rem_dr$findElement(using = "xpath","//a[@href='api/v1/auth/login']")
  web_elem$clickElement()
  Sys.sleep(1)
  
  # o2r Admin/ Editor / User
  
  # Click on 'Admin'
  web_elem$findElement(using="xpath","/html/body/div/div[2]/form[3]/button")
  web_elem$clickElement()
  
  # Get cookie and decoding
  cookie<-URLdecode(webElem$getAllCookies()[[1]]$value)
  Sys.sleep(1)
  Sys.setenv(COOKIE=cookie)
  Sys.setenv(ENDPOINT="https://o2r.uni-muenster.de/api/v1/")
  
  # Closing Selenium session
  rem_dr$close()
  rm(r)
}
```


#### Who am I ?

Function to verify authentication status. 

```{r whoami}

whoami<-function(){
  cookie<-Sys.getenv("COOKIE")
  endpoint<-Sys.getenv("ENDPOINT")
  response<-GET(url=paste0(endpoint,"auth/whoami"),
               accept_json(),
               set_cookies(connect.sid=cookie),
               enconde="multipart"
)
  return(response)
}
```

### 2. ERC / Workspace Upload

The test differentiates between two types of uploads (either workspace or a complete ERC) and 3 origins (Direct, Zenodo and Sciebo). The following 3 functions (one for each origin) upload the workspaces and the complete ERC for either a `localtest` or a `remotetest`. The function *requires a previous Service authentication* to define `ENDPOINT` and `COOKIE`.

#### [Local upload](http://o2r.info/api/compendium/upload/)

Upload a research workspace or full compendium as a compressed `.zip`.

```{r local_upload}

# local_upload 
### Parameters
# compendium -> The archive file
# content_type -> Form of the archive ('compendium' or 'workspace')

local_upload<-function(compendium,content_type){
  file<-upload_file(toString(compendium))
  endpoint<-Sys.getenv("ENDPOINT")
  cookie<-Sys.getenv("COOKIE")
  response<-POST(url=paste0(endpoint,"compendium"),
                 body=list(
                   compendium=file,
                   content_type=content_type),
                 accept_json(),
                 set_cookies(connect.sid=cookie),
                 enconde="multipart"
  )
  return(response)
}  
```

#### [Public Share - Sciebo](http://o2r.info/api/compendium/public_share/#sciebo)

Upload a research workspace or full compendium as a compressed `.zip` from a public share on [Sciebo](https://sciebo.de/).

```{r public_share_sciebo}
### Parameters
# public_share_sciebo
# share_url -> The Sciebo link to the public share
# content_type -> Form of archive ('compendium' or 'workspace')
# path -> Path to a subdirectory or a zip file in the public share

public_share_sciebo<-function(share_url,path,content_type){

  endpoint<-Sys.getenv("ENDPOINT")
  cookie<-Sys.getenv("COOKIE")
  
  response<-POST(url=paste0(endpoint,"compendium"),
                 body=list(
                   share_url=share_url,
                   content_type=content_type,
                   path=path),
                 accept_json(),
                 set_cookies(connect.sid=cookie),
                 enconde="multipart"
  )
  return(response)
}
```

#### [Public Share - Zenodo](http://o2r.info/api/compendium/public_share/#zenodo)

Upload a research workspace or full compendium as a compressed `.zip` from a public share on [Zenodo](https://zenodo.org/) using the `Zenodo Record ID`.

```{r public_share_zenodo}
# public_share_zenodo
### Parameters
# zenodo_record_id -> The Zenodo record id
# content_type -> Form of archive ('compendium' or 'workspace')

public_share_zenodo<-function(zenodo_record_id,content_type){

  endpoint<-Sys.getenv("ENDPOINT")
  cookie<-Sys.getenv("COOKIE")
  
  response<-POST(url=paste0(endpoint,"compendium"),
                 body=list(
                   content_type=content_type,
                   zenodo_record_id=zenodo_record_id),
                 accept_json(),
                 set_cookies(connect.sid=cookie),
                 enconde="multipart"
  )
  return(response)
}
```

### 3. Metadata editing / Publish compendium [(Candidate Process)](http://o2r.info/api/compendium/candidate/#candidate-process)

After uploading a compendium a [succesful metadata update](http://o2r.info/api/compendium/metadata/#update-metadata) is required to make it publicly available. The following code reproduce the metadata extracting and metadata update required to publish the compendium.

```{r publish_compendium}

# publish_compendium
### Parameters
# id -> compendium id

publish_compendium <- function(id){
  
  endpoint<-Sys.getenv("ENDPOINT")
  cookie<-Sys.getenv("COOKIE")
  
  url_metadata<-paste0(endpoint,"compendium/",id,"/metadata")
  print(url_metadata)
  print("Extracting metadata...")
 
  # Extract metadata
  response <- GET(url=url_metadata,
                  accept_json(),
                  set_cookies(connect.sid = cookie))
  
  print("Printing metadata")
  print(response)
  
  metadata <-  content(response, as = "text")
  metadata <- str_sub(string = metadata,start = str_locate(string = metadata, pattern = "\\{\"o2r\"")[[1]],end = str_length(metadata) - 1)
  
  print("printing... metadata")
  print(metadata)
  
  # Update metadata "Candidate process"
  response_update <- PUT(url=url_metadata,
                         body = metadata,
                         content_type_json(),
                         accept_json(),
                         set_cookies(connect.sid = cookie))
  return(response_update)
}
```


### 4. [Compendium execution](http://o2r.info/api/job/)

#### Execution job

Run the analysis (Execution job) in a published compendium. 

```{r execute_compendium}
# execute_compendium
### Parameters
# id -> compendium id

execute_compendium<-function(id){
  
  endpoint<-Sys.getenv("ENDPOINT")
  cookie<-Sys.getenv("COOKIE")
  
  response <- POST(url = paste0(endpoint, "job"),
                   body = list(compendium_id = id[[1]]),
                   accept_json(),
                   set_cookies(connect.sid = cookie))
  return(response)
  
}
```

#### Job Status

Checks the status of a job

```{r job_status}
# job_status
### Parameters
# job_id -> Job id

job_status <- function(job_id) {
  endpoint<-Sys.getenv("ENDPOINT")
  cookie<-Sys.getenv("COOKIE")
  print(paste0(endpoint, "job/", job_id))
  response <- GET(url = paste0(endpoint, "job/", job_id,"?steps=all"),
                  accept_json(),
                  # authenticate even if not needed to not destroy cookie caching
                  set_cookies(connect.sid = cookie))
  return(response)
}
```

## ERC examination session

For the examination session it is required an already published compendium  `id`. Aditionally the examination session can be configured adding `start_pause` representing a time before starting the session, `login_pause` representing the time the user usually spends login into the service and `execution_pause` corresponding to the time the user takes before executing a job.

```{r examination_session}

# examination_session
### Parameters
# id -> compendium id
# start_pause -> Time before starting the session (in seconds)
# login_pause -> Login-in time (in seconds)
# execution_pause -> Time before executing a job (in seconds)

examination_session<-function(id,
                              start_pause=0,
                              login_pause=0,
                              execution_pause=0){
  
  ### Starting pause
  print(paste0("I'm going to take ", start_pause," seconds before starting to check this Compendium"))
  Sys.sleep(start_pause)
  
  ### Login pause simulation
  print(paste0("Ok, I'm going to login ! This usually takes ", login_pause," seconds"))
  Sys.sleep(login_pause)
  print("Looks like I'm already in !")
  whoami()
  
  ### Running the analysis
  print(paste0("Let me check the paper ", execution_pause," seconds before running the analysis !"))
  Sys.sleep(execution_pause)
  print(paste0("Looks interesting ! Let's run an analysis of Compendium: ",id))
  job_executing<-execute_compendium(toString(id))
  
  response<-job_executing$status
  job_id<-content(job_executing)

  if (response==200){
    print(paste0("Looks like it is already executing the job: ",job_id))
  }
  
  result<-list(x=job_executing)
  return(result)
}

```

## ERC creation session

For the examination session it is required a series of configurations. Firstly, `upload_origin` correspond to the ERC / Workspace Upload origin (Local, Zenodo or Sciebo),`upload_origin` corresponds to the form of archive ('compendium' or 'workspace'), `source` and `source_path` provide the source information required to upload the workspaces depending the origin. Other configurations representing interaction with the platform can be modified: `start_pause` representing a time before starting the session, `login_pause` representing the time the user usually spends login into the service,`uploading_pause` representing the time before uploading the files to the service, `metaedit_pause` representing the time required to check the metadata before publishing and
`execution_pause` corresponding to the time the user takes before executing a job.


```{r creation_session}
# creation_session
### Parameters
# upload_origin -> Type of upload ("Local","Zenodo","Sciebo")
# content_type -> Form of archive ('compendium' or 'workspace')
# source -> For "Direct" upload path to zip file, for "Zenodo" zenodo_record_id OR for "Sciebo" The Sciebo link to the public share.
# source_path -> ONLY for Sciebo upload path to a subdirectory or a zip file in the public share otherwise empty.
# start_pause -> Time before starting the session (in seconds)
# login_pause -> Login-in time (in seconds)
# uploading_pause -> Time before uploading the files (in seconds)
# metaedit_pause -> Time required to check the metadata before publishing (in seconds)
# execution_pause -> Time before executing a job (in seconds)

creation_session<-function(upload_origin,
                          content_type,
                          source,
                          source_path,
                          start_pause=0,
                          login_pause=0,
                          upload_pause=0,
                          metaedit_pause=0,
                          execution_pause=0){
  
  ### STARTING PAUSE
  
  print(paste0("I'm going to take ", start_pause," seconds before starting to upload this Compendium"))
  Sys.sleep(start_pause)
  
  ### LOGIN Simulation
  
  print(paste0("Ok, I'm going to login ! This usually takes ", login_pause," seconds"))
  Sys.sleep(login_pause)
  print("Looks like I'm already in !")
  whoami()
  
  ### Uploading 
  print("OK, time to upload an Compendium !")
  print(paste0("Let me check the files " , upload_pause, " seconds before uploading !"))
  
  if(upload_origin=="Local"){
    upload<-local_upload(compendium = source,content_type = toString(content_type))
    print("Uploading from my computer !")
  } else if (upload_origin=="Sciebo"){
    upload<-public_share_sciebo(share_url = toString(source),path=toString(source_path),content_type=toString(content_type))
    print("Uploading from Sciebo !")
  } else if (upload_origin=="Zenodo"){
    upload<-public_share_zenodo(zenodo_record_id = source,content_type = toString(content_type))
    print("Uploading from Zenodo !")
  } else {
    print("There is a problem with the source file ! It must be Local / Sciebo or Zenodo")
    return(NULL)
  }
  
  ### Candidate process
  print(paste0("Now I have to check the metadata before publishing, this would probably take me ", metaedit_pause ," seconds" ))
  
  print(upload)
  
  id<-toString(content(upload))
  print(paste0("My compendium id is: ", id))
  publish<-publish_compendium(id)
  
  print(paste0("My compendium ", id, " is already public !"))
  
  ### Execution process
  
  Sys.sleep(execution_pause)
  print(paste0("Looks interesting ! Let's run an analysis of Compendium: ",id))
  job_execute<-execute_compendium(toString(id))
  
  response<-job_execute$status
  
  job_id<-content(job_execute)

  if (response==200){
    print(paste0("Looks like it is already executing the job: ",job_id))
  }
  
  job_status<-job_status(job_id)

  result<-list(x=upload,y=publish,z=job_execute,w=job_status)
  return(result)
}

```
**TO DO: ** Job execution (Check status, websockets !)

# Setting up the test environment

Before starting the test scenarios some dummy ERC workspaces need to be available. For the **examiniation scenario** it is required 10 Compendium already available on the o2r service (i.e publications that are going to be examined) and for the **creation sessions** 3 ERC workspaces.

The following method copy a dummy ERC workspace from a `source` folder and creates one (or multiple) new modified dummy on a `target` folder. In specific, every new dummy workspace includes a shell script that use the `Sleep` command to simulate computation time of an ERC. The number of dummies and the `SLEEP` time is configurable throught the `dummy_times` parameter. This parameter is expected to be of numeric class and every number (integers only) represents the sleeping time of a new dummy ERC workspace. In consequence `dummy_times = c(10,100)` will result in **two** new ERC workspaces with 10 and 100 seconds of `SLEEP` time respectively. 

```{r create_dummies}
# create_dummies
### Parameters
# dummy_times -> Numeric, for example: c(1,1,2,3,5,8,13)
# source -> path to the original dummy 
# target -> path to target folder 

create_dummies<-function(dummy_times,source,target){
  
  # Checks if the file source exists and that the format of dummy_times is correct
  # If check fails exits with error message
  
  if (!is.numeric(dummy_times) | !file.exists(source)){
    return(cat("ERROR: Seconds data type should be integer\n"," and 'Dummy' folder should exist"))
  }
  
  # Configuration of target, source and dataFiles to copy.
  target<-file.path(target)
  source<-file.path(source)
  
  dataFiles<-dir(source)
  
  # Loop over dummy times and creates a new dummy
  
  for (seconds in dummy_times){
    
    # Skip if seconds is not an integer
    
    if (seconds%%1!=0){
      cat("ERROR: ### Seconds must be an integer number ! ###\n")
      next
    }
    
    # Create target folder if it does not already exist
    dir.create(target,recursive=TRUE,showWarnings = FALSE)
    
    # Create dummy folder
    # Generates a ramdom ID to identify dummy folder
    dummy_id <- do.call(paste0, replicate(5, sample(LETTERS, 1), FALSE))
    
    # Name of the folder includes the sleep time (in seconds) and ID
    target_dummy_name<-paste0("dummy_",seconds,"_seconds_",dummy_id)
    target_dummy<-(file.path(target,target_dummy_name))
    dir.create(target_dummy)
    
    # Read files from original dummy and copy them into new dummy folder
    file.copy(file.path(source,dataFiles),target_dummy,overwrite = TRUE)
  
    # Modify dummy 
     
     dockerfile<-readLines(file.path(target_dummy,"Dockerfile"),-1)
     dockerfile[4]<-paste0("ENV SHEEP ", seconds)
     writeLines(dockerfile,file.path(target_dummy,"Dockerfile"))
     
    # Zip dummy
     
     zip(target_dummy,file.path(target_dummy,dataFiles),flag="-jq")
     unlink(target_dummy,recursive=TRUE)
     
     cat("*Your dummy ERC [",dummy_id,"] has been succesfully created on ", target_dummy,".zip *\n","*It will sleep: ",seconds," seconds* \n",sep="")
    # 
  }
}

```


# LoadTest

```{r test, eval=FALSE}

#Test for Examination sessions

#get_cookie_remote()

#Examination<-data.frame(read.csv("ExaminationScenario.csv", header = TRUE, sep = ","))
#resultsExamination<-mcmapply(readerSession,Examination$Compendium_ID,Examination$Start_Pause,Examination$Login_time,Examination$Execution_Pause,mc.silent = FALSE)

#Test for Creation Sessions

#Creation<-data.frame(read.csv("CreationScenario.csv",header=TRUE,sep=","))

#resultsCreationa<-mcmapply(creation_session,Creation$upload_origin,Creation$content_type,Creation$source,Creation$source_path,Creation$start_pause,Creation$login_pause,Creation$upload_pause,Creation$metaedit_pause,Creation$execution_pause)

```


# Report

```{r}

creation_dataframe<-function(list){
  
  compendium_id<-content(list[[1]])
  names(compendium_id)<-"compendium_id"
  
  upload_times<-list[[1]]$times
  upload_status<-rep(list[[1]]$status,6)
  names(upload_status)<-paste0('status5upload_',names(upload_times))
  names(upload_times)<-paste0('time5upload_',names(upload_times))
  
  candidate_times<-list[[2]]$times
  candidate_status<-rep(list[[2]]$status,6)
  names(candidate_status)<-paste0('status5candidate_',names(candidate_times))
  names(candidate_times)<-paste0('time5candidate_',names(candidate_times))

  job_id<-content(list[[3]])
  execution_times<-list[[3]]$times
  execution_status<-rep(list[[3]]$status,6)
  names(execution_status)<-paste0('status5execution_',names(execution_times))
  names(execution_times)<-paste0('time5execution_',names(execution_times))
  
  job_status<-content(list[[4]])$steps
  job_steps_time_names<-paste0("time5job_",names(job_status))
  job_steps_status_names<-paste0("status5job_",names(job_status))
  
  job_steps_start<-sapply(jobStatus,"[",1 )%>% sapply(function(x) as.POSIXct(x,format="%Y-%m-%dT%H:%M:%OS"))
  job_steps_end<-sapply(jobStatus,"[",2) %>% sapply(function(x) as.POSIXct(x,format="%Y-%m-%dT%H:%M:%OS"))
  job_steps_time<-job_steps_end-job_steps_start
  names(job_steps_time)<-job_steps_time_names
  
  job_steps_status<-sapply(jobStatus,"[",3)
  names(job_steps_status)<-job_steps_status_names
  
  info<-c(compendium_id,job_id,upload_times,upload_status,candidate_times,candidate_status,execution_times,execution_status,job_steps_time,job_steps_status)
  
  df<-data.frame(list(info))
  df<-df%>%pivot_longer(-c(compendium_id,job_id),names_to=c(".value","process_step"),names_sep="5")
  return(df)
}

```

